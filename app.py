# -*- coding: utf-8 -*-
"""CRAG_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HyCjEOpzCFtNT-llCyyVBrwP0ABbY-8v
"""


# Commented out IPython magic to ensure Python compatibility.
# %pip install -U langchain-text-splitters

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U langchain-hub

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U langchain langchain-community langchain-openai chromadb tiktoken

import os
os.environ["LANGCHAIN_TRACING_V2"]=""
os.environ["LANGCHAIN_ENDPOINT"]=""
os.environ["LANGCHAIN_API_KEY"]=""
os.environ["LANGCHAIN_PROJECT"]=""
os.environ["OPENAI_API_KEY"] = ""
os.environ["TAVILY_API_KEY"] = ""

import langchain
print(langchain.__version__)

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

urls = [
    "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
    "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",
]

docs = [WebBaseLoader(url).load() for url in urls]
docs_list = [item for sublist in docs for item in sublist]

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=250, chunk_overlap=0
)
doc_splits = text_splitter.split_documents(docs_list)

vectorstore = Chroma.from_documents(
    documents=doc_splits,
    collection_name="rag-chroma1",
    embedding=OpenAIEmbeddings(model="text-embedding-3-small"),
)

retriever = vectorstore.as_retriever()

# Retrieval Grader

from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from typing import Literal
from langchain_openai import ChatOpenAI

# pydantic data model

class GradeDocuments(BaseModel):
  """Binary score for relevance check on retrieved documents."""

  binary_score: str = Field(
      description="Documents are relevant to the question, 'yes' or 'no'"
  )

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
structured_llm_grader = llm.with_structured_output(GradeDocuments)

system = """You are a grader assessing relevance of a retrieved document to a user question. \n
    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \n
    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."""

grade_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "Retrieved document: \n\n {document} \n\n User question: {question}"),
    ]
)

retrieval_grader = grade_prompt | structured_llm_grader

question = "agent memory"

docs = retriever.invoke(question)

doc_txt = docs[0].page_content

print(retrieval_grader.invoke({"question": question, "document": doc_txt}))

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Answer using only the context. If not in context, say you don't know."),
    ("human", "Context:\n{context}\n\nQuestion:\n{question}")
])

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

def format_docs(docs):
    return "\n\n".join(d.page_content for d in docs)

rag_chain = prompt | llm | StrOutputParser()

generation = rag_chain.invoke({
    "context": format_docs(docs),
    "question": question
})

print(generation)

### Question Re-writer

# LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# Prompt
system = """You are a question re-writer that converts an input question to a better version that is optimized \n
     for web search. Look at the input and try to reason about the underlying semantic intent / meaning."""
re_write_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        (
            "human",
            "Here is the initial question: \n\n {question} \n Formulate an improved question.",
        ),
    ]
)

question_rewriter = re_write_prompt | llm | StrOutputParser()
question_rewriter.invoke({"question": question})

from langchain_community.tools.tavily_search import TavilySearchResults

web_search_tool = TavilySearchResults(k=3)

# create the graph

from typing import TypedDict, List

class GraphState(TypedDict):
  question: str
  documents: List[str]
  generation: str
  web_search: str

def retrieve(state: GraphState) -> GraphState:
    print("------RETRIEVE-------")
    question = state["question"]
    documents = retriever.invoke(question)  # âœ… updated API
    return {"question": question, "documents": documents}

def generate(state:GraphState) -> GraphState:
  print("--------GENERATE---------")
  question = state["question"]
  documents = state["documents"]

  generation = rag_chain.invoke({"question": question, "context": documents})
  return {"question": question, "documents": documents, "generation": generation}

def grade_documents(state:GraphState) -> GraphState:
  print("----- Grading the Documents --------")
  question = state["question"]
  documents = state["documents"]

  # score each doc

  filtered_docs = []
  web_search = "No"

  for d in documents:
    score = retrieval_grader.invoke({"question": question, "document": d.page_content})
    grade = score.binary_score

    if grade == "yes":
      print("---- Grade: Document Relevant")
      filtered_docs.append(d)
    else:
      print("---- Grade: Document Not Relevant")
      web_search = "Yes"
      continue
  return {"question": question, "documents": filtered_docs, "web_search": web_search}

from langchain_core.documents import Document

def web_search(state: GraphState) -> GraphState:
    print("----- web search --------")
    question = state["question"]
    documents = state["documents"]

    docs = web_search_tool.invoke({"query": question})
    web_results = "\n".join([d["content"] for d in docs])

    web_doc = Document(page_content=web_results)
    documents.append(web_doc)

    return {"question": question, "documents": documents}

def transform_query(state:GraphState) -> GraphState:
  print("-------- Tranform Query --------")
  question = state["question"]
  documents = state["documents"]

  better_question = question_rewriter.invoke({"question": question})
  return {"question": better_question, "documents": documents}

def decide_to_generate(state:GraphState):
  print("-------- assess graded documents--------")
  web_search = state["web_search"]

  if web_search == "Yes":
    print("----- all documents are not relevant to the question, so transform query-----")
    return "transform_query"
  else:
    print("----- all documents are relevant to the question, so generate -----")
    return "generate"

from langgraph.graph import START, END, StateGraph

workflow = StateGraph(GraphState)

workflow.add_node("retrieve", retrieve)
workflow.add_node("grade_documents", grade_documents)
workflow.add_node("generate", generate)
workflow.add_node("transform_query", transform_query)
workflow.add_node("web_search_node", web_search)

workflow.add_edge(START, "retrieve")
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "transform_query": "transform_query",
        "generate": "generate"
    }
)
workflow.add_edge("transform_query", "web_search_node")
workflow.add_edge("web_search_node", "generate")
workflow.add_edge("generate", END)

app = workflow.compile()

from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))

from pprint import pprint

# Run
inputs = {"question": "What are the types of agent memory?"}
for output in app.stream(inputs):
    for key, value in output.items():
        pprint(f"Node '{key}':")
    pprint("\n---\n")

# Final generation
pprint(value["generation"])

from pprint import pprint

# Run
inputs = {"question": "How does the AlphaCodium paper work?"}
for output in app.stream(inputs):
    for key, value in output.items():
        # Node
        pprint(f"Node '{key}':")
        # Optional: print full state at each node
        pprint(value["documents"], indent=2, width=80, depth=None)
    pprint("\n---\n")

# Final generation




import streamlit as st
from pprint import pprint

from PIL import Image
import io


# Streamlit UI code
st.title("CRAG Question-Answering System")

question_input = st.text_input("Enter your question:")

def run_crag(question):
    inputs = {"question": question}
    # Directly invoke app to get the result (no need for app.stream)
    output = app.invoke(inputs)  # Assuming app.invoke gives you the final answer directly
    
    # If "generation" is in the result, return it; else return a default message
    return output.get("generation", "No generation found")

if question_input:
    result = run_crag(question_input)  # Get the result from the CRAG system
    st.write("Answer:", result)  # Display the final answer in the app


# Generate the workflow graph image
graph_image = app.get_graph().draw_mermaid_png()  # This generates the graph image

# Load the image from binary
image = Image.open(io.BytesIO(graph_image))  # Load image from binary

# Get original image dimensions
width, height = image.size

# Set the new width and calculate height while maintaining the aspect ratio
new_width = 200
new_height = int(new_width * (height / width))  # Maintain aspect ratio

# Display the resized image with only the width argument
st.image(graph_image, caption="Workflow Graph", width=new_width)
